{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"id\", \"date\",\"query\",\"user\",\"text\"]\n",
    "train_data = pd.read_csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\", \n",
    "                         header=None, \n",
    "                         names=cols, \n",
    "                         engine=\"python\", \n",
    "                         encoding=\"latin1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"trainingandtestdata/testdata.manual.2009.06.14.csv\",\n",
    "                       header=None,\n",
    "                       names=cols,\n",
    "                       engine=\"python\",\n",
    "                       encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  id                          date    query      user  \\\n",
       "0          4   3  Mon May 11 03:17:40 UTC 2009  kindle2    tpryan   \n",
       "1          4   4  Mon May 11 03:18:03 UTC 2009  kindle2    vcu451   \n",
       "2          4   5  Mon May 11 03:18:54 UTC 2009  kindle2    chadfu   \n",
       "3          4   6  Mon May 11 03:19:04 UTC 2009  kindle2     SIX15   \n",
       "4          4   7  Mon May 11 03:21:41 UTC 2009  kindle2  yamarama   \n",
       "\n",
       "                                                text  \n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.sentiment.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"id\", \"date\", \"query\", \"user\"], # don't forget to run data = train_data before!\n",
    "          axis=1,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", \" \", tweet)\n",
    "    tweet = re.sub(\"https?://[A-Za-z0-9./]+\", \" \",tweet)\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", \" \", tweet)\n",
    "    tweet = re.sub(\" +\", \" \", tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tf2\\lib\\site-packages\\bs4\\__init__.py:336: MarkupResemblesLocatorWarning: \" i just received my G8 viola exam.. and its... well... .. disappointing.. :\\..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "D:\\Anaconda\\envs\\tf2\\lib\\site-packages\\bs4\\__init__.py:336: MarkupResemblesLocatorWarning: \"E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\../  \\../\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    }
   ],
   "source": [
    "data_clean = [clean_tweet(tweet) for tweet in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = data.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels[data_labels == 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    data_clean, target_vocab_size=2**16\n",
    ")\n",
    "\n",
    "data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(sentence) for sentence in data_inputs])\n",
    "data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n",
    "                                                           value=0,\n",
    "                                                           padding=\"post\",\n",
    "                                                           maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = np.random.randint(0, 800000, 8000)\n",
    "test_idx = np.concatenate((test_idx, test_idx+800000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = data_inputs[test_idx]\n",
    "test_labels = data_labels[test_idx]\n",
    "train_inputs = np.delete(data_inputs, test_idx, axis=0)\n",
    "train_labels = np.delete(data_labels, test_idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, \n",
    "                vocab_size,\n",
    "                embed_dim=128,\n",
    "                nb_filters = 50,\n",
    "                FFN_units = 512,\n",
    "                nb_classes = 2,\n",
    "                dropout_rate = 0.1,\n",
    "                training=False,\n",
    "                name=\"dcnn\"):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        self.bigram = layers.Conv1D(filters = nb_filters, kernel_size=2,\n",
    "                                   padding=\"valid\",\n",
    "                                   activation = \"relu\")\n",
    "        self.pool_1 = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.trigram = layers.Conv1D(filters = nb_filters, kernel_size=3,\n",
    "                                   padding=\"valid\",\n",
    "                                   activation = \"relu\")\n",
    "        self.pool_2 = layers.GlobalMaxPool1D()\n",
    "        self.fourgram = layers.Conv1D(filters = nb_filters, kernel_size=4,\n",
    "                                   padding=\"valid\",\n",
    "                                   activation = \"relu\")\n",
    "        self.pool_3 = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units = FFN_units, activation='relu')\n",
    "        self.dropout = layers.Dropout(rate = dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units = 1, activation='sigmoid')\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units = nb_classes, acitvation=\"softmax\")\n",
    "            \n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool_1(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool_2(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool_3(x_3)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the model!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configuration of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "EMB_DIM = 200\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = len(set(train_labels))\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "BATCH_SIZE=32\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
    "           embed_dim=EMB_DIM,\n",
    "           nb_filters=NB_FILTERS,\n",
    "           FFN_units=FFN_UNITS,\n",
    "           nb_classes=NB_CLASSES,\n",
    "           dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss='binary_crossentropy',\n",
    "                optimizer = 'adam',\n",
    "                metrics=['accuracy'])\n",
    "else:\n",
    "    Dcnn.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "                optimizer='adam',\n",
    "                metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Checkpoint Restored!!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./ckpt/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest Checkpoint Restored!!!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dcnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  13108000  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              multiple                  40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  60100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            multiple                  80100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  77056     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  257       \n",
      "=================================================================\n",
      "Total params: 13,365,613\n",
      "Trainable params: 13,365,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Dcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1584082 samples\n",
      "Epoch 1/5\n",
      "1584082/1584082 [==============================] - 44791s 28ms/sample - loss: 0.3982 - accuracy: 0.8201\n",
      "Epoch 2/5\n",
      "1584082/1584082 [==============================] - 9678s 6ms/sample - loss: 0.3336 - accuracy: 0.8566\n",
      "Epoch 3/5\n",
      "1584082/1584082 [==============================] - 8685s 5ms/sample - loss: 0.2824 - accuracy: 0.8828\n",
      "Epoch 4/5\n",
      "1584082/1584082 [==============================] - 7721s 5ms/sample - loss: 0.2295 - accuracy: 0.9077\n",
      "Epoch 5/5\n",
      "1584082/1584082 [==============================] - 7676s 5ms/sample - loss: 0.1845 - accuracy: 0.9273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./ckpt/ckpt-1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn.fit(train_inputs, train_labels, batch_size=BATCH_SIZE,\n",
    "        epochs=NB_EPOCHS)\n",
    "ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 10s 652us/sample - loss: 0.1331 - accuracy: 0.9507\n",
      "[0.1330859230812639, 0.9506875]\n"
     ]
    }
   ],
   "source": [
    "results = Dcnn.evaluate(test_inputs, test_labels, batch_size = BATCH_SIZE)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "if Dcnn(np.array([tokenizer.encode(\"You are cute!\")]), training=False).numpy()>0.6:\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22985332]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn(np.array([tokenizer.encode(\"I love you but I do not think this will work out.\")]), training=False).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
